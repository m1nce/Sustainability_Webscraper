{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wefel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#from transformers import BertTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "#from happytransformer import HappyTextClassification\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import textwrap\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import textwrap\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "CUSTOM_CONFIG_ID = os.getenv(\"CUSTOM_CONFIG_ID\")\n",
    "\n",
    "# FEEL FREE TO CHANGE num_results. It controls how many URLs it searches for.\n",
    "# If your code is running slow or you are using CPU, maybe reduce number of results.\n",
    ", try reducing number of results.\n",
    "def bing_search(query, api_key, custom_config_id, num_results=10, excluded_site=''):\n",
    "    search_query = f\"{query} -site:{excluded_site}\" if excluded_site else query\n",
    "    search_url = f\"https://api.bing.microsoft.com/v7.0/custom/search?q={search_query}&customconfig={custom_config_id}\"\n",
    "    headers = {'Ocp-Apim-Subscription-Key': api_key}\n",
    "    params = {'count': num_results}\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve search results: {response.status_code}, {response.text}\")\n",
    "        return []\n",
    "    results = response.json().get('webPages', {}).get('value', [])\n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return []\n",
    "    urls = [result['url'] for result in results]\n",
    "    print(f\"Found URLs: {urls}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"EnvironmentalBERT\": (\"Vinoth24/environmental_claims\", None),\n",
    "    \"SocialBERT\": (\"ESGBERT/SocialBERT-social\", None),\n",
    "    \"GovernanceBERT\": (\"ESGBERT/GovernanceBERT-governance\", None)\n",
    "}\n",
    "\n",
    "# Initialize models and tokenizers and move models to GPU\n",
    "for key, (model_name, _) in model_info.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to('cuda')\n",
    "    model_info[key] = (model, tokenizer)\n",
    "\n",
    "\n",
    "# If no GPU:\n",
    "# models = {\n",
    "#     \"EnvironmentalBERT\": HappyTextClassification(\"BERT\", \"Vinoth24/environmental_claims\"),\n",
    "#     \"SocialBERT\": HappyTextClassification(\"BERT\", \"ESGBERT/SocialBERT-social\"),\n",
    "#     \"GovernanceBERT\": HappyTextClassification(\"BERT\", \"ESGBERT/GovernanceBERT-governance\")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify_content(content):\n",
    "    content = re.sub(r'\\s+', ' ', content)  # Normalize whitespace\n",
    "    sentences = sent_tokenize(content)\n",
    "    classified_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:\n",
    "            # Use text wrapping to manage long lines\n",
    "            wrapped_text = textwrap.fill(sentence, width=150)\n",
    "            for line in wrapped_text.split('\\n'):\n",
    "                # Check each model for classification\n",
    "                for model_name, (model, tokenizer) in model_info.items():\n",
    "                    inputs = tokenizer(line, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                        score, predicted_class = probs.max(1)\n",
    "                        if score.item() > 0.6 and predicted_class.item() == 1:\n",
    "                            clean_line = ' '.join(line.split())\n",
    "                            classified_sentences.append(f\"{model_name}: {clean_line}\")\n",
    "                            break  # Stop checking other models if one has classified the line already\n",
    "\n",
    "    return '\\n'.join(classified_sentences)\n",
    "\n",
    "\n",
    "# def process_and_classify_content(content):\n",
    "#     content = re.sub(r'\\s+', ' ', content)  # Normalize whitespace\n",
    "#     sentences = sent_tokenize(content)\n",
    "#     classified_sentences = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence = sentence.strip()\n",
    "#         if sentence:\n",
    "#             # Use text wrapping to avoid excessively long lines\n",
    "#             wrapped_text = textwrap.fill(sentence, width=80)\n",
    "#             for line in wrapped_text.split('\\n'):\n",
    "#                 # Check each model for classification\n",
    "#                 for model_name, model in models.items():\n",
    "#                     result = model.classify_text(line)\n",
    "#                     if result.label == \"LABEL_1\" and result.score > 0.5:\n",
    "#                         clean_line = ' '.join(line.split())\n",
    "#                         classified_sentences.append(f\"{model_name}: {clean_line}\")\n",
    "#                         break  # Stop checking other models if one has classified the line already\n",
    "\n",
    "    # return '\\n'.join(classified_sentences)\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=20)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                return soup.get_text()\n",
    "    except requests.Timeout:\n",
    "        print(f\"Timeout occurred for URL: {url}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def search_and_scrape(query, company_website, max_threads=10):\n",
    "    urls = bing_search(query, API_KEY, CUSTOM_CONFIG_ID, excluded_site=company_website)\n",
    "    with open('scraped_data.txt', 'w', encoding='utf-8') as file:\n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            future_to_url = {executor.submit(fetch_content, url): url for url in urls}\n",
    "            for future in as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                try:\n",
    "                    content = future.result()\n",
    "                    if content:\n",
    "                        processed_content = process_and_classify_content(content)\n",
    "                        if processed_content:\n",
    "                            file.write(f\"URL: {url}\\nContent:\\n{processed_content}\\n\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = \"nike\"\n",
    "company_website = \"nike.com\"\n",
    "print(f\"Searching: intext:\\\"{company_name}\\\" company sustainability\")\n",
    "search_and_scrape(f\"intext:\\\"{company_name}\\\" company sustainability\", company_website)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
