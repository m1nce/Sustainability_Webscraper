{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wefel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import concurrent.futures\n",
    "from happytransformer import HappyTextClassification\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "CSE_ID = os.getenv(\"CSE_ID\")\n",
    "\n",
    "# Function to perform a Google search and return a list of URLs\n",
    "def google_search(query, api_key, cse_id, num_results=10):\n",
    "    search_url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cse_id}&num={num_results}\"\n",
    "    response = requests.get(search_url)\n",
    "    results = response.json().get('items', [])\n",
    "    urls = [result['link'] for result in results]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/19/2024 15:56:40 - INFO - happytransformer.happy_transformer -   Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "happy_class = HappyTextClassification(model_type=\"BERT\", model_name=\"Vinoth24/environmental_claims\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_scrape(query, company_website):\n",
    "    urls = google_search(query, API_KEY, CSE_ID)\n",
    "    urls_to_scrape = [url for url in urls if company_website.lower() not in url.lower() and not any(substring in url for substring in ['.xlsx', 'sitemap', '/download', 'List', 'list'])]\n",
    "\n",
    "    \n",
    "    with open('scraped_data.txt', 'w', encoding='utf-8') as file:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_to_url = {executor.submit(fetch_content, url): url for url in urls_to_scrape}\n",
    "            for future in concurrent.futures.as_completed(future_to_url):\n",
    "                url = future_to_url[future]\n",
    "                print(f\"Processing: {url}\")\n",
    "                try:\n",
    "                    content = future.result()\n",
    "                    if content:\n",
    "                        # Process and classify the content, then write to file if applicable\n",
    "                        processed_content = process_and_classify_content(content)\n",
    "                        if processed_content:\n",
    "                            file.write(f\"URL: {url}\\nContent:\\n{processed_content}\\n\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "def process_and_classify_content(content):\n",
    "    # Replace multiple spaces with a single space\n",
    "    content = re.sub(r'\\s+', ' ', content)\n",
    "    \n",
    "    sentences = sent_tokenize(content)\n",
    "    classified_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()  # Strip leading and trailing whitespaces\n",
    "        if sentence:  # Check if sentence is not empty after stripping\n",
    "            # Break down the sentence into smaller chunks if necessary, maintaining sentence integrity\n",
    "            chunks = [sentence[i:i+512] for i in range(0, len(sentence), 512)] if len(sentence) > 512 else [sentence]\n",
    "            for chunk in chunks:\n",
    "                result = happy_class.classify_text(chunk)\n",
    "                if result.label == \"LABEL_1\" and result.score > 0.5:\n",
    "                    # Further clean each chunk to ensure no leading/trailing spaces and normalize inner spaces\n",
    "                    clean_chunk = ' '.join(chunk.split())\n",
    "                    classified_sentences.append(clean_chunk)\n",
    "    return '\\n'.join(classified_sentences)\n",
    "\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        with requests.Session() as session:  # Use a Session for connection pooling\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                return soup.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching: intext:\"nike\" company sustainability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/19/2024 15:56:40 - INFO - happytransformer.happy_transformer -   Moving model to cpu\n",
      "03/19/2024 15:56:40 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: https://sustainabilitymag.com/articles/nike-making-strides-towards-net-zero-with-sustainable-foam\n",
      "Processing: https://goodonyou.eco/how-ethical-is-nike/\n",
      "Processing: https://www.dazeddigital.com/fashion/article/52679/1/what-you-need-to-know-about-nike-sustainability-goals-2021-microsite\n",
      "Processing: https://www.weavabel.com/blog/is-nike-sustainable-focusing-on-a-brighter-future\n",
      "Processing: https://directory.goodonyou.eco/brand/nike\n"
     ]
    }
   ],
   "source": [
    "company_name = \"nike\"\n",
    "company_website = \"nike.com\" # Currently best if website is input without \"https:/\" and \"www.\"\n",
    "print(f\"Searching: intext:\\\"{company_name}\\\" company sustainability\")\n",
    "search_and_scrape(f\"intext:\\\"{company_name}\\\" company sustainability\", company_website)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
